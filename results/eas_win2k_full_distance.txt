
 Arguments:
agent_name: full
autolen: True
batch_size: 256
dis_dim: 50
domain: win2k
early_stop: 50
epochs: 200
gpu_fraction: 0.2
hist_len: 8
image_dim: 8
image_padding: 1
learning_rate: 0.001
load_indices: True
max_train_doms: 6400
metric: succ
min_epochs: 20
model_dim: 50
num_actions: 2
num_filters: 32
num_grams: 9
num_pos: 37
num_words: 500
pos_dim: 50
result_dir: results/eas_win2k_full_distance.txt
save_model: True
start_test_dom: 7200
start_valid_dom: 6400
state_dim: 3
sub_sampling: True
train_mode: eas
use_cnn: True
use_ngrams: True
weight_dir: weights/eas_win2k_full.h5
word2vec: <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x1a226fed50>
word_dim: 50

epoch: 1	 loss: 235.937490	 train_f1: 0.342605
		 valid_f1: 0.212867	 test_f1: 0.262948
epoch: 2	 loss: 137.116521	 train_f1: 0.506600
		 valid_f1: 0.229972	 test_f1: 0.265060
epoch: 3	 loss: 103.915415	 train_f1: 0.543492
		 valid_f1: 0.260042	 test_f1: 0.286303
epoch: 4	 loss: 73.762068	 train_f1: 0.577498
		 valid_f1: 0.284141	 test_f1: 0.324786
epoch: 5	 loss: 47.109293	 train_f1: 0.632586
		 valid_f1: 0.304158	 test_f1: 0.348663
epoch: 6	 loss: 30.194974	 train_f1: 0.650752
		 valid_f1: 0.309859	 test_f1: 0.357672
epoch: 7	 loss: 20.822694	 train_f1: 0.656005
epoch: 8	 loss: 15.313206	 train_f1: 0.657087
epoch: 9	 loss: 12.430497	 train_f1: 0.658839
epoch: 10	 loss: 10.171911	 train_f1: 0.660580
epoch: 11	 loss: 8.346706	 train_f1: 0.661664
epoch: 12	 loss: 7.017572	 train_f1: 0.662753
epoch: 13	 loss: 5.997465	 train_f1: 0.664712
epoch: 14	 loss: 5.146486	 train_f1: 0.665147
epoch: 15	 loss: 4.458053	 train_f1: 0.665147
		 valid_f1: 0.310532	 test_f1: 0.363445
epoch: 16	 loss: 3.941629	 train_f1: 0.666016
epoch: 17	 loss: 3.480299	 train_f1: 0.666016
epoch: 18	 loss: 3.112165	 train_f1: 0.666450
epoch: 19	 loss: 2.797127	 train_f1: 0.666884
epoch: 20	 loss: 2.558416	 train_f1: 0.666884
		 valid_f1: 0.312704	 test_f1: 0.362683
epoch: 21	 loss: 2.338307	 train_f1: 0.666884
epoch: 22	 loss: 2.148485	 train_f1: 0.666884
epoch: 23	 loss: 1.989603	 train_f1: 0.666884
epoch: 24	 loss: 1.851822	 train_f1: 0.666884
epoch: 25	 loss: 1.725848	 train_f1: 0.666884
epoch: 26	 loss: 1.610494	 train_f1: 0.666884
epoch: 27	 loss: 1.500577	 train_f1: 0.666884
epoch: 28	 loss: 1.410666	 train_f1: 0.666884
epoch: 29	 loss: 1.302262	 train_f1: 0.666884
epoch: 30	 loss: 1.227901	 train_f1: 0.666884
epoch: 31	 loss: 1.135979	 train_f1: 0.666884
epoch: 32	 loss: 1.082012	 train_f1: 0.666884
epoch: 33	 loss: 0.996627	 train_f1: 0.666884
epoch: 34	 loss: 0.910091	 train_f1: 0.666884
epoch: 35	 loss: 0.874346	 train_f1: 0.666884
epoch: 36	 loss: 0.800287	 train_f1: 0.666884
epoch: 37	 loss: 0.727593	 train_f1: 0.666884
epoch: 38	 loss: 0.686374	 train_f1: 0.666884
epoch: 39	 loss: 0.621747	 train_f1: 0.667317
epoch: 40	 loss: 0.588677	 train_f1: 0.667317
epoch: 41	 loss: 0.550125	 train_f1: 0.667534
epoch: 42	 loss: 0.500361	 train_f1: 0.667534
epoch: 43	 loss: 0.511774	 train_f1: 0.667534
epoch: 44	 loss: 0.447482	 train_f1: 0.667534
epoch: 45	 loss: 0.425410	 train_f1: 0.667534
		 valid_f1: 0.313384	 test_f1: 0.363064
epoch: 46	 loss: 0.444171	 train_f1: 0.667534
epoch: 47	 loss: 0.390369	 train_f1: 0.667534
epoch: 48	 loss: 0.409251	 train_f1: 0.667534
epoch: 49	 loss: 0.349692	 train_f1: 0.667101
epoch: 50	 loss: 0.331239	 train_f1: 0.667534
epoch: 51	 loss: 0.379801	 train_f1: 0.667534
epoch: 52	 loss: 0.377287	 train_f1: 0.667317
epoch: 53	 loss: 0.413119	 train_f1: 0.667101
epoch: 54	 loss: 0.292683	 train_f1: 0.667534
epoch: 55	 loss: 0.255146	 train_f1: 0.667534
epoch: 56	 loss: 0.238161	 train_f1: 0.667534
epoch: 57	 loss: 0.235196	 train_f1: 0.667534
epoch: 58	 loss: 0.279221	 train_f1: 0.667534
epoch: 59	 loss: 0.351620	 train_f1: 0.667317
epoch: 60	 loss: 0.517805	 train_f1: 0.667101
epoch: 61	 loss: 0.488017	 train_f1: 0.667317
epoch: 62	 loss: 0.453599	 train_f1: 0.666884
epoch: 63	 loss: 0.278342	 train_f1: 0.667101
epoch: 64	 loss: 0.285051	 train_f1: 0.667317
epoch: 65	 loss: 0.498677	 train_f1: 0.666667
epoch: 66	 loss: 0.687268	 train_f1: 0.667317
epoch: 67	 loss: 0.593745	 train_f1: 0.666884
epoch: 68	 loss: 0.394296	 train_f1: 0.667101
epoch: 69	 loss: 0.300323	 train_f1: 0.667317
epoch: 70	 loss: 0.403653	 train_f1: 0.666667
epoch: 71	 loss: 0.593504	 train_f1: 0.667317
